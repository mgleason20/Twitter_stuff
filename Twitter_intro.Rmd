---
title: "Twitter - Text Mining, Sentiment Analysis, and future directions"
author: "Morgan"
date: "11/28/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Helpful resources: 
- cran.r-project.org for your package of interest
- https://cran.r-project.org/web/packages/rtweet/index.html (intro)
- Stack overflow
- R Studio Community
- github.com


Shout out to: 
-Nathan Young (DePaul Psych Science) (https://github.com/nthnyng/RMD_repo/blob/master/Twitter_example.Rmd)
- Aaron Mamula, an analyst at NOAA (National Oceanic and Atmospheric Administration) https://github.com/aaronmams/rHD-Vignette-Text-Mining/blob/master/Twitter-Scraping-Example.Rmd)

```{r}
library(rtweet) # for harvesting tweets

library(tm) # for text mining
library(dplyr) 
library(tidyr)
library(data.table)
library(ggplot2)
library(ggthemes)
library(here)
library(tidyverse)
library(tidytext)
library(hms)#date and time
library(lubridate)#date and time
library(glue)# Data Wrangling and Visualization
library(cowplot)
library(magrittr)
library(plotly)
library(tidyverse)
library(tidytext)
library(widyr)# Data Wrangling and Visualization
library(tidytext)# Text Mining
library(tm)
library(wordcloud)# Text Mining
library(igraph)# Network Analysis
library(networkD3)# Network Visualization (D3.js)
library(ngram)# Network Visualization (D3.js)
library(syuzhet)# Sentiment analysis
library(ggraph)# misc libraries nathan uses
library(reshape2)
library(devtools)
library(SnowballC)# misc libraries Nathan uses
```

Setup: I recommend creating a developer account with Twitter in order to access data through their public API (this takes about 1 hour). Once you have created a developer account, you need to register an App. Once you've done this, you will get: an API key, an API secret, an access token, an access token secret (I've saved mine in a csv file and loaded it in to R to streamline the process, as per Aaron Mamula's tutorial).

These steps allow you to establish a connection between R and Twitter's API.

```{r}
library(rtweet)

creds <- read.csv(here('twitter_creds.csv'), stringsAsFactors=FALSE)

api_key <- creds$ï..api_key
api_secret_key <- creds$api_secret
access_token <- creds$access_token 
access_secret <- creds$access_token_secret

token <- create_token(
  app = "meisterfish",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_secret)
```

Let's search for some tweets that reference the POTUS account.
```{r}
t <- Sys.time()
## search for some tweets referencing the @POTUS account.
rt <- search_tweets(
  "@POTUS",n=100,include_rts = FALSE,retryonratelimit=F)
Sys.time() -t
head(rt)
```

We can look at the screen name and text to get a higher-level view.
```{r}
rt %>% dplyr::select(screen_name,text)
```

Let's combine what we've done so far.
```{r}
# look at the popular hashtags used by people mentioning POTUS
potus <- search_tweets(
  "@POTUS",n=1000,include_rts = FALSE,retryonratelimit=F)
ht <- potus %>% dplyr::select(created_at,screen_name,hashtags)
# hashtags are stored in a list tidyr has an answer for that
ht <- unnest(ht,cols=c(hashtags))
# probably too many to visualize so maybe just look at the top 20
top.20 <- ht %>% filter(is.na(hashtags)==F) %>% 
             group_by(hashtags) %>% summarise(count=n()) %>% 
               arrange(-count) %>% filter(row_number() < 21) %>% 
                 arrange(count) %>% mutate(hashtags=factor(hashtags,levels=hashtags))
```

Now that our list is organized in descending order by count, we can create a plot to visualize the data in ggplot.
```{r}
ggplot(top.20,aes(x=hashtags,y=count)) + geom_bar(stat='identity') + 
  coord_flip() + theme_fivethirtyeight() + 
    ggtitle(label="Popular hashtags in tweets about POTUS",
            subtitle="hashtag use last 10 days") +
     theme(plot.title = element_text(size = 14, face = "bold"),
           plot.subtitle = element_text(face = "italic"))
```

Intro provided by Nathan Young c/o DUSC Lab at DePaul University 9/24/2020
I've modified/used some of his code below (most of which is in the intro to rtweet on github)
https://github.com/nthnyng/RMD_repo.git (Nathans Github)
https://github.com/ropensci/rtweet  (Rtweet materials)


Let's search for followers!
```{r}
mgf <- get_followers("Mgtweets2")
mgf
```

Donald Trump's followers...?
```{r}
## there is no limit to the number of followers (friends limited to 5000)
## here is an example for if we wanted to get a listing of all of Donald Trumps 48.8 million followers -- but it would take longer than 5 days to work!!

## get all of trump's followers
rdt <- get_followers("realdonaldtrump", n = 5e7, retryonratelimit = TRUE)
```

Search_tweets function using hashtags
```{r}
## search for 1000 tweets using the hashtags #counteveryvote
vt <- search_tweets(
  "#counteveryvote", n = 500, include_rts = FALSE
)
```

Let's look at the time series of the tweets we found above
```{r}
vt %>%
  ts_plot("3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of #counteveryvote Twitter statuses from past 9 days",
    subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```

Searching tweets using specific words (or combinations of words)
```{r}
## search for 10,000 tweets containing the word Trump
tt <- search_tweets(
  "Trump", n = 10000, retryonratelimit = TRUE
)

#you can also search phrases or combinations of words. rt <- search_tweets(q = "rstats", n = 1000). default = 100 tweets, but can set n higher 

## examples:
rt <- search_tweets(q = "rstats")
## search for a phrase
rt <- search_tweets(q = "data science")
## search for multiple keywords
rt <- search_tweets(q = "rstats AND python")
```

```{r}
ht <- tt %>% dplyr::select(created_at,screen_name,hashtags)
# hashtags are stored in a list tidyr has an answer for that
ht <- unnest(ht,cols=c(hashtags))
# probably too many to visualize so maybe just look at the top 30
top.20 <- ht %>% filter(is.na(hashtags)==F) %>% 
             group_by(hashtags) %>% summarise(count=n()) %>% 
               arrange(-count) %>% filter(row_number() < 21) %>% 
                 arrange(count) %>% mutate(hashtags=factor(hashtags,levels=hashtags))
```
```{r}
ggplot(top.20,aes(x=hashtags,y=count)) + geom_bar(stat='identity') + 
  coord_flip() + theme_fivethirtyeight() + 
    ggtitle(label="Popular hashtags in tweets about Trump",
            subtitle="hashtag use last 10 days") +
     theme(plot.title = element_text(size = 14, face = "bold"),
           plot.subtitle = element_text(face = "italic"))
```

Grab tweets from a users account
```{r}
#Grabs the last 3200 tweets from the president's account
potus <- get_timeline(c("potus"), n = 3200)
```

Reformat date
```{r}
#make sure date is in a readable format year month day; hour minute second
potus$created_at <- ymd_hms(potus$created_at)
```

Plot POTUS tweets
```{r}
#create rounded time column to the hour
potus2 <- potus %>%
  mutate(created_at_r = round_date(created_at, "day"))
#plot tweet frequency
potus2 %>%
  count(created_at_r) %>% 
  ggplot(aes(x = created_at_r, y = n)) +
    theme_light() +
    geom_line() +
    xlab(label = 'Date') +
    ylab(label = NULL) +
    ggtitle(label = 'Number of Tweets per Minute')
```

Sentiment Analysis - streamline and format text
```{r}
potus2 %>%
  #convert text to lower case
  mutate(Text = text %>% str_to_lower()) %>% 
  #remove unwanted characters
  mutate(Text = Text %>% str_remove_all(pattern = "\\n"),
         Text = Text %>% str_remove_all(pattern = '&amp'),
         Text = Text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*'),
         Text = Text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*'),
         Text = Text %>% str_remove_all(pattern = 'https'),
         Text = Text %>% str_remove_all(pattern = 'http'),
         #remove hashtags
         Text = Text %>% str_remove_all(pattern = '#[a-z,A-Z]*'),
         #remove accounts
         Text = Text %>% str_remove_all(pattern = '@[a-z,A-Z]*'),
         #remove retweets
         Text = Text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: '),
         Text = Text %>% str_remove(pattern = '^(rt)'),
         Text = Text %>% str_remove_all(pattern = '\\_')) -> potus2
         
# Replace accents. 
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
potus2 %>% 
  mutate(Text = chartr(old = names(replacement.list) %>% str_c(collapse = ''), 
                       new = replacement.list %>% str_c(collapse = ''),
                       x = Text)) -> potus2
```

Create clean text column in data frame
```{r}
#create clean text column in data frame
corpus <- Corpus(x = VectorSource(x = potus2$Text))
corpus %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords('en')) %>%
  tm_map(PlainTextDocument) -> potus_text
potus2 <- potus2 %>%
  mutate(Text_corpus = potus_text[[1]]$content) 
```

Extract hastags
```{r}
#function to extract only hashtags
GetHashtags <- function(tweet) {
  hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>% 
    as.character()
  
  hashtag.string <- NA
  
  if (length(hashtag.vector) > 0) {
    
    hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
    
  } 
  return(hashtag.string)
}
```

```{r}
#get hashtags using above function
hashtags.df <- tibble(Hashtags = potus$text %>% map_chr(.f = ~ GetHashtags(tweet = .x)))
```

```{r}
#bind text and hashtag data together
potus2 %<>% bind_cols(hashtags.df)
```

```{r}
stopwords.df <- tibble(
  word = c(stopwords(kind = 'en'))
)
words.df <- potus2 %>%
  unnest_tokens(input = Text, output = word) %>%
  anti_join(y = stopwords.df, by = 'word')
word_count <- words.df %>% count(word, sort = TRUE)
```

```{r}
word_count %>% 
  # Set count threshold. 
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  theme_light() + 
  geom_col(fill = 'black', alpha = 0.8) +
  xlab(NULL) +
  coord_flip() +
  ggtitle(label = 'Top Word Count')
```

```{r}
wordcloud(
  words = word_count$word, 
  freq = word_count$n, 
  min.freq = 100, 
  colors = brewer.pal(8, 'Dark2')
)
```

```{r}
#grab only text, put it in a vector object
potus_textonly <- as.vector(potus2$Text_corpus)
potus_textonly2 <- str_replace_all(potus2$Text_corpus,"[^[:graph:]]", " ") 
#put the text vector into a function that scores the text based on different sentiment categories. This on matches words with a positive / negative text corpus. Others such as Vadar & SentiStrength use more complex algorithms.
sentiment_dat <- get_nrc_sentiment(potus_textonly2)
#combine the extracted sentiment data with the original dataset
potus_senti <- bind_cols(potus2, sentiment_dat)
```

How pos/neg are the tweets we extracted?
```{r}
potus_senti %>%
  summarise(positivity = mean(positive), negativity = mean(negative)) %>%
  gather(valence_type, score, positivity:negativity) %>%
  ggplot() +
  geom_bar(aes(x = valence_type, y = score, fill = valence_type), stat = "identity") +
  xlab("Valence Type") +
  ylab("Sentiment Score") +
  guides(fill = FALSE) +
  theme_bw() 
```

search for 1,000 tweets sent from the US?
```{r}
usat <- search_tweets(
  "lang:en", geocode = lookup_coords("usa"), n = 1000
)

## create lat/lng variables using all available tweet and profile geo-location data
usat <- lat_lng(usat)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
with(usat, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```

Discover what's currently trending in Chicago?
```{r}
chitown <- get_trends("chicago")
```

